---
title: Analysis
description:
toc: true
featuredVideo:
featuredImage: https://thumbor.forbes.com/thumbor/fit-in/900x510/https://www.forbes.com/health/wp-content/uploads/2021/08/Untitled-design-2021-08-27T123144.738.png
draft: false
---


<div id="TOC">

</div>

<p>This comes from the file <code>content/analysis.Rmd</code>.</p>
<p>We describe here our detailed data analysis.</p>
<div id="section-1-motivation-of-data-analysis" class="section level2">
<h2>Section 1: Motivation of Data Analysis</h2>
<p>As the United States population grows and develops, there will be a greater need for information on the nation’s general health. Our plan for the project is to evaluate differences in health outcomes by race while considering covariates like gender, age, and time of data collection.</p>
<p>Our response variable used to measure health outcomes is Mean Arterial Pressure (MAP), which combines both Diastolic and Systolic Blood Pressures into one value. We chose to use Blood Pressure as the dependent variable because it is often considered one of the best ways to measure overall health and possible risk factors.</p>
<p>Our exploratory data analysis (EDA) begins with the goal of investigating the relevant independent variables and their correlation with the dependent variable. We perform our exploratory data analysis by performing in-depth analysis on the following independent variables selected from the dataset:</p>
<ul>
<li>Year</li>
<li>Age</li>
<li>Gender</li>
<li>Ethnicity</li>
<li>Government Health Expenditure.</li>
</ul>
</div>
<div id="section-2-variable-analysis" class="section level2">
<h2>Section 2: Variable Analysis</h2>
<p>We start the variable analysis by looking at the correlation matrix. A correlation matrix displays the correlation between all the possible pairs of values. From the correlation matrix, we can see that Age is moderately correlated and Ethnicity, Gender, Year are weakly correlated. After looking at the correlation matrix, we took a deeper dive into each variable.</p>
<p><img src="/analysis_files/figure-html/corr_matrix-1.png" width="672" /></p>
<div id="variable-year" class="section level3">
<h3>2.1 Variable: Year</h3>
<p>From the boxplot of MAP of each year, we can see that MAP varies by year. Additionally, Median MAP increases over the years. This suggests us to consider the effect of Year on MAP and prompts us to look into other variables that might influence MAP.</p>
<p><img src="/analysis_files/figure-html/year_boxplot-1.png" width="672" /></p>
</div>
<div id="variable-age" class="section level3">
<h3>2.2 Variable: Age</h3>
<p>Age is a continuous variable that is most correlated with MAP. From the smoothed regression line plot of <code>Age</code> vs. <code>MAP</code>, we can see that MAP increases as Age increases as it is commonly known that people’s blood pressure increases as they age. Interestingly, MAP starts to decrease for people over 65 years old. We believe that this could be due to people with high blood pressure taking medicine to lower their blood pressure.</p>
<p><img src="/analysis_files/figure-html/age_lineplot-1.png" width="672" /></p>
<p>For our predicted model and model evaluation later, we choose to use Age as the X variable because it is the only continuous variable in the model, and the linear relationship between Age and MAP is moderate. Using Age as X allows us to further look into the relationship between MAP and other categorical variables.</p>
</div>
<div id="variable-gender" class="section level3">
<h3>2.3 Variable: Gender</h3>
<p>We use a scatter plot to investigate the relationship between Gender and MAP. Since the original dataset has around 60,000 data points, we choose Year 2018, the most recent year, to help us see the pattern more clearly.</p>
<p><img src="/analysis_files/figure-html/scatter_plot_gd-1.png" width="672" /></p>
<p>In the scatter plot, data points of female are colored in red and male are colored in green. From the graph, we can see that females generally have lower MAP. This suggests a possible interaction between <code>Gender</code> and <code>MAP</code>, which we will take into account in the models later.</p>
</div>
<div id="variable-ethnicity" class="section level3">
<h3>2.4 Variable: Ethnicity</h3>
<p>Similar to the scatter plot for Gender, we plot the scatter plot of Ethnicity vs. MAP in Year 2018. Because data points for each ethnicity are not randomly distributed and seem to show some patterns, we will consider the interaction between <code>Ethnicity</code> and <code>MAP</code> as well.</p>
<p><img src="/analysis_files/figure-html/scatter_plot_eth-1.png" width="672" /></p>
</div>
<div id="variable-government-health-expenditure" class="section level3">
<h3>2.5 Variable: Government Health Expenditure</h3>
<p>In addition to our original dataset, we used a secondary dataset of Government Health Expenditure in percentage. The graph of Government Health Expenditure over the year shows that government spending is increasing over the year. We plot the graph of Median MAP over the year as well to see if there is a possible correlation between the two variables.</p>
<p><img src="/analysis_files/figure-html/MAPvsEx_sidebyside-1.png" width="672" /></p>
<p>To explore the relationship between Government Health Expenditure and Median MAP, we use a simple linear regression model to evaluate the correlation. The p-value of variable <code>GovernmentHealthExpenditure</code> is 0.198, suggesting that it is not significant. Therefore, we will not consider this variable in the model later.</p>
<pre class="r"><code>MAPEx_model &lt;- lm(MedianMAP ~ GovernmentHealthExpenditure, data = MAPvsEx)
summary(MAPEx_model)$coefficients</code></pre>
<pre><code>##                               Estimate Std. Error   t value     Pr(&gt;|t|)
## (Intercept)                 82.5800262  3.2779110 25.192882 6.597953e-09
## GovernmentHealthExpenditure  0.2372216  0.1693857  1.400482 1.989405e-01</code></pre>
</div>
</div>
<div id="modeling-and-inferences" class="section level2">
<h2>3. Modeling and Inferences</h2>
<p>We start by dividing the dataset into a training set and a testing set using the <code>createDataPartition</code> function in the <code>caret</code> package. The training set contains 75% of the data (43,319 rows) and the testing set contains the rest of the data (14,439 rows). Then, we ran numerous multiple linear regressions on training data.</p>
<pre><code>##   Training Testing
## 1    43319   14439</code></pre>
<div id="multiple-linear-regression" class="section level3">
<h3>3.1 Multiple Linear Regression</h3>
<div id="a-model" class="section level4">
<h4>3.1. a) Model</h4>
<p>The first model we consider is a multiple linear regression model. The independent variables are <code>Ethnicity</code>, <code>Year</code>, <code>Age</code>, and <code>Gender</code>, and the dependent variable is <code>MAP</code>.
<br>
<br></p>
<pre class="r"><code>model &lt;- lm(MAP ~ Ethnicity + factor(Year) + Age + Gender, data = train.modelData)
summary(model) </code></pre>
<pre><code>## 
## Call:
## lm(formula = MAP ~ Ethnicity + factor(Year) + Age + Gender, data = train.modelData)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -64.027  -7.251  -0.537   6.700  70.636 
## 
## Coefficients:
##                            Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)               79.125437   0.259965 304.370  &lt; 2e-16 ***
## EthnicityMexican American -3.381548   0.183062 -18.472  &lt; 2e-16 ***
## EthnicityOther Hispanic   -3.142503   0.240121 -13.087  &lt; 2e-16 ***
## EthnicityOther Race       -2.271176   0.231478  -9.812  &lt; 2e-16 ***
## EthnicityWhite            -3.426828   0.154350 -22.202  &lt; 2e-16 ***
## factor(Year)2002          -0.567880   0.264704  -2.145 0.031931 *  
## factor(Year)2004          -1.635519   0.269296  -6.073 1.26e-09 ***
## factor(Year)2006          -2.080764   0.268311  -7.755 9.02e-15 ***
## factor(Year)2008          -1.352917   0.263346  -5.137 2.80e-07 ***
## factor(Year)2010          -2.086260   0.260275  -8.016 1.12e-15 ***
## factor(Year)2012          -1.044502   0.269302  -3.879 0.000105 ***
## factor(Year)2014          -1.685170   0.263852  -6.387 1.71e-10 ***
## factor(Year)2016          -1.111809   0.266041  -4.179 2.93e-05 ***
## factor(Year)2018           0.812366   0.270861   2.999 0.002708 ** 
## Age                        0.252832   0.002929  86.320  &lt; 2e-16 ***
## Gendermale                 2.587526   0.115925  22.321  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 12.06 on 43303 degrees of freedom
## Multiple R-squared:  0.1701, Adjusted R-squared:  0.1698 
## F-statistic: 591.8 on 15 and 43303 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
<div id="b-model-fit" class="section level4">
<h4>3.1. b) Model Fit</h4>
<p>We use root-mean-square error (RMSE) and R-squared (R2) to evaluate model performance. We compare predictions of MAP based on independent variables to the real MAP values in the testing set to see how well the model fits. RMSE measures the average difference between values predicted by a model and the actual values. The lower the value of the RMSE, the better the model is. R2 measures the strength of the relationship between the linear model and the dependent variables on a 0-1 scale.</p>
<p>The results of RMSE and R2 are shown below. R2 = 0.169 means that 16.9% of the variance for the dependent variable MAP is explained by the independent variables in the MLR model. The result is not perfect, but we believe it is still a good model for MAP, and we will use other statistical methods to improve the model.</p>
<pre><code>##     RMSE        R2
## 1 12.029 0.1693604</code></pre>
</div>
<div id="c-variable-selection" class="section level4">
<h4>3.1. c) Variable Selection</h4>
<ul>
<li>VIF</li>
</ul>
<p>One common problem of regression model is multicollinearity. Multicollinearity may hide significant variables, change the sign of them and result in an increase in the variability of the estimation. To detect multicollinearity in the regression model, we use Variance Inflation Factor (VIF) to measure the amount of multicollinearity. To calculate VIF, we use <code>vif</code> function in <code>car</code> package.</p>
<pre class="r"><code>vif(model)</code></pre>
<pre><code>##                  GVIF Df GVIF^(1/(2*Df))
## Ethnicity    1.127406  4        1.015103
## factor(Year) 1.098781  9        1.005247
## Age          1.045412  1        1.022454
## Gender       1.000712  1        1.000356</code></pre>
<p>Since all variables have VIF &lt; 5, multicollinearity is not a concern in our model.</p>
<ul>
<li>AIC</li>
</ul>
<p>In addition, we can also use Akaike information criterion (AIC) to perform further feature selection. AIC can be used to compare different possible models and determine which one is the best fit for the data.</p>
<pre class="r"><code># perform AIC to eliminate insignificant variables
stepAIC(model, direction =&quot;both&quot;)</code></pre>
<pre><code>## Start:  AIC=215711.9
## MAP ~ Ethnicity + factor(Year) + Age + Gender
## 
##                Df Sum of Sq     RSS    AIC
## &lt;none&gt;                      6294718 215712
## - factor(Year)  9     32646 6327364 215918
## - Gender        1     72423 6367141 216205
## - Ethnicity     4     81679 6376397 216262
## - Age           1   1083128 7377846 222588</code></pre>
<pre><code>## 
## Call:
## lm(formula = MAP ~ Ethnicity + factor(Year) + Age + Gender, data = train.modelData)
## 
## Coefficients:
##               (Intercept)  EthnicityMexican American  
##                   79.1254                    -3.3815  
##   EthnicityOther Hispanic        EthnicityOther Race  
##                   -3.1425                    -2.2712  
##            EthnicityWhite           factor(Year)2002  
##                   -3.4268                    -0.5679  
##          factor(Year)2004           factor(Year)2006  
##                   -1.6355                    -2.0808  
##          factor(Year)2008           factor(Year)2010  
##                   -1.3529                    -2.0863  
##          factor(Year)2012           factor(Year)2014  
##                   -1.0445                    -1.6852  
##          factor(Year)2016           factor(Year)2018  
##                   -1.1118                     0.8124  
##                       Age                 Gendermale  
##                    0.2528                     2.5875</code></pre>
<p>After using <code>stepAIC</code> function from the <code>MASS</code> package on our multiple linear regression model, no variables were eliminated. Thus, we come to the conclusion that all the variables are significant to the model.</p>
</div>
</div>
<div id="multiple-linear-regression-with-interactions" class="section level3">
<h3>3.2 Multiple Linear Regression with Interactions</h3>
<p>Interaction effect means that two or more features/variables combined have a significantly larger effect on a feature as compared to the sum of the individual variables alone.</p>
<div id="a-multiple-linear-regression-with-ethnicity-interacts-with-age" class="section level4">
<h4>3.2 a) Multiple Linear Regression with Ethnicity Interacts with Age</h4>
<p>Since the scatter plot in EDA shows possible interaction between <code>Ethnicity</code> and <code>Age</code>, we use <code>Ethnicity*Age</code> to model the interaction effect.</p>
<pre class="r"><code># multiple linear regression model with interaction terms 
model_interaction_eth &lt;- lm(MAP ~ Ethnicity + factor(Year) + Age + Gender +  
                        Ethnicity*Age, data = train.modelData)</code></pre>
<p>We use RMSE and R2 to measure model performance.</p>
<pre><code>##       RMSE        R2
## 1 11.98485 0.1754982</code></pre>
<p><img src="/analysis_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>The graph of predicted MAP with and without the interaction term shows that there is an interaction between the two variables.</p>
</div>
<div id="b-multiple-linear-regression-with-gender-interacts-with-age" class="section level4">
<h4>3.2 b) Multiple Linear Regression with Gender Interacts with Age</h4>
<p>Similarly, the scatter plot in EDA shows possible interaction between <code>Gender</code> and <code>Age</code>, so we use <code>Gender*Age</code> to model the interaction effect.</p>
<pre class="r"><code>model_interaction_gd &lt;- lm(MAP ~ Ethnicity + factor(Year) + Age + Gender +  
                        Gender*Age, data = train.modelData)</code></pre>
<p>We use RMSE and R2 to measure model performance.</p>
<pre><code>##       RMSE        R2
## 1 11.99895 0.1735145</code></pre>
<p><img src="/analysis_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>The graph of predicted MAP with and without the interaction term shows that there is an interaction between the two variables.</p>
</div>
<div id="c-multiple-linear-regression-with-ethnicity-and-gender-interact-with-age" class="section level4">
<h4>3.2 c) Multiple Linear Regression with Ethnicity and Gender Interact with Age</h4>
<p>Since both interaction terms are significant based on the graphs, we use <code>Ethnicity*Age</code> and <code>Gender*Age</code> to model the interaction effect in our final model.</p>
<pre class="r"><code>model_interaction_both &lt;- lm(MAP ~ Ethnicity + factor(Year) + Age + Gender +  
                        Gender*Age + Ethnicity*Age, data = train.modelData)</code></pre>
<p>The RMSE and R2 results of this model are shown below. In this model, RMSE = 11.95303, which is lower than RMSE in the MLR model (RMSE = 12.029), MLR model with only Gender interaction (RMSE = 11.99895), and MLR model with only Ethnicity interaction (RMSE = 11.98485). In addition, R2 = 0.1799 means that in this model, 17.99% of the variance for the dependent variable MAP is explained by the independent variables in the MLR model. This R2 is also better than all the other MLR models we have used above.</p>
<pre><code>##       RMSE       R2
## 1 11.95303 0.179887</code></pre>
<p>Among the models used, the multiple linear regression model with <code>Ethnicity*Age</code> and <code>Gender*Age</code> as interaction terms has the lowest RMSE and the highest R2. Therefore, this will be our final model of choice.</p>
</div>
</div>
<div id="logistic-regression" class="section level3">
<h3>3.3 Logistic Regression</h3>
<p>In addition to multiple linear regression model, we also try logistic regression model to predict the probability of a person having hypertension (<code>HTN</code> = 1) based on their <code>Ethnicity</code>, <code>Age</code>, and <code>Gender</code>. The graph of the logistic regression model is shown below.</p>
<pre class="r"><code>log_model &lt;- glm(Hypertension ~ Ethnicity + Age + Gender, data = modelData, family = binomial)</code></pre>
<pre><code>## fitting null model for pseudo-r2</code></pre>
<pre><code>##           llh       llhNull            G2      McFadden          r2ML 
## -2.152911e+04 -2.337150e+04  3.684792e+03  7.883087e-02  6.180464e-02 
##          r2CU 
##  1.113946e-01</code></pre>
<p>To evaluate the model performance, we use the <code>pR2</code> function in the <code>pscl</code> package to calculate pseudo-r2. Our pseudo-r2 is 0.0788, suggesting that model is poor fit. Therefore, we decide to use the multiple linear regression model with interaction terms instead.</p>
</div>
<div id="conclusion" class="section level3">
<h3>3.4 Conclusion</h3>
<p>From the graph of MLR model with Ethnicity as interaction term, we come to the conclusion that health outcomes (measured by MAP) differ by race since the regression line of Ethnicity Black is above all the other regression line. Other factors such as gender and age also have an effect on an individual’s health outcomes, and using our model, we can predict a person’s MAP.</p>
</div>
<div id="flaws-and-limitations" class="section level3">
<h3>4. Flaws and Limitations</h3>
<ul>
<li>Flaws</li>
</ul>
<p>In the linear regression model, there are four assumptions that need to be satisfied: linearity, homoscedasticity, independence, and normality. One possible flaw of our model is the normality assumption might be violated. The normality assumption states that for any fixed value of X, Y is normally distributed.
<img src="/analysis_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p>From the graph of the distribution of the dependent variable, MAP, we can see that MAP is slightly right-skewed even after adjusting for MAP using formula. The violation of the normality assumption is attributed by the skewed nature of the dependent variable, Blood Pressure. Blood Pressure is slightly skewed to the right due to the high proportion of people with Blood Pressure values in the range above 140 mmHg. Normality assumption violation will affect the estimates of the standard error and the confidence interval, and hence the significance of the independent variables.</p>
<ul>
<li>Limitations</li>
</ul>
<p>Our model has limited predictive power due to the limited selection of predictors we can use. The dataset we used does not have geographic information and the data points are anonymous. We cannot incorporate geographic datasets as a secondary data source due to anonymity of the responders. Therefore, useful geographic predictors cannot be used to account for the differences.</p>
</div>
</div>
